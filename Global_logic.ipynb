{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed644d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Warning [3C90, f=1]: B/H/T lengths differ (B=20714352, H=20714352, T=647) -> truncating to 647\n",
      "[3C90, f=1] windows added: 567 (series length 647)\n",
      "Warning [3C90, f=2]: B/H/T lengths differ (B=29203344, H=29203344, T=1459) -> truncating to 1459\n",
      "[3C90, f=2] windows added: 1379 (series length 1459)\n",
      "Warning [3C90, f=3]: B/H/T lengths differ (B=28861632, H=28861632, T=2252) -> truncating to 2252\n",
      "[3C90, f=3] windows added: 2172 (series length 2252)\n",
      "Warning [3C90, f=4]: B/H/T lengths differ (B=26064780, H=26064780, T=3252) -> truncating to 3252\n",
      "[3C90, f=4] windows added: 3172 (series length 3252)\n",
      "Warning [3C90, f=5]: B/H/T lengths differ (B=16371152, H=16371152, T=3269) -> truncating to 3269\n",
      "[3C90, f=5] windows added: 3189 (series length 3269)\n",
      "Warning [3C90, f=6]: B/H/T lengths differ (B=10612800, H=10612800, T=3300) -> truncating to 3300\n",
      "[3C90, f=6] windows added: 3220 (series length 3300)\n",
      "Warning [3C90, f=7]: B/H/T lengths differ (B=6229440, H=6229440, T=3090) -> truncating to 3090\n",
      "[3C90, f=7] windows added: 3010 (series length 3090)\n",
      "Warning [3C94, f=1]: B/H/T lengths differ (B=35473728, H=35473728, T=1108) -> truncating to 1108\n",
      "[3C94, f=1] windows added: 1028 (series length 1108)\n",
      "Warning [3C94, f=2]: B/H/T lengths differ (B=6765408, H=6765408, T=338) -> truncating to 338\n",
      "[3C94, f=2] windows added: 258 (series length 338)\n",
      "Warning [3C94, f=3]: B/H/T lengths differ (B=10778256, H=10778256, T=841) -> truncating to 841\n",
      "[3C94, f=3] windows added: 761 (series length 841)\n",
      "Warning [3C94, f=4]: B/H/T lengths differ (B=16502885, H=16502885, T=2059) -> truncating to 2059\n",
      "[3C94, f=4] windows added: 1979 (series length 2059)\n",
      "Warning [3C94, f=5]: B/H/T lengths differ (B=9349936, H=9349936, T=1867) -> truncating to 1867\n",
      "[3C94, f=5] windows added: 1787 (series length 1867)\n",
      "Warning [3C94, f=6]: B/H/T lengths differ (B=6753600, H=6753600, T=2100) -> truncating to 2100\n",
      "[3C94, f=6] windows added: 2020 (series length 2100)\n",
      "Warning [3C94, f=7]: B/H/T lengths differ (B=4233600, H=4233600, T=2100) -> truncating to 2100\n",
      "[3C94, f=7] windows added: 2020 (series length 2100)\n",
      "Warning [3E6, f=1]: B/H/T lengths differ (B=28654320, H=28654320, T=895) -> truncating to 895\n",
      "[3E6, f=1] windows added: 815 (series length 895)\n",
      "Warning [3E6, f=2]: B/H/T lengths differ (B=41933520, H=41933520, T=2095) -> truncating to 2095\n",
      "[3E6, f=2] windows added: 2015 (series length 2095)\n",
      "Warning [3E6, f=3]: B/H/T lengths differ (B=2845152, H=2845152, T=222) -> truncating to 222\n",
      "[3E6, f=3] windows added: 142 (series length 222)\n",
      "Warning [3E6, f=4]: B/H/T lengths differ (B=10996580, H=10996580, T=1372) -> truncating to 1372\n",
      "[3E6, f=4] windows added: 1292 (series length 1372)\n",
      "Warning [3E6, f=5]: B/H/T lengths differ (B=9014400, H=9014400, T=1800) -> truncating to 1800\n",
      "[3E6, f=5] windows added: 1720 (series length 1800)\n",
      "Warning [3E6, f=6]: B/H/T lengths differ (B=6753600, H=6753600, T=2100) -> truncating to 2100\n",
      "[3E6, f=6] windows added: 2020 (series length 2100)\n",
      "Warning [3E6, f=7]: B/H/T lengths differ (B=4231584, H=4231584, T=2099) -> truncating to 2099\n",
      "[3E6, f=7] windows added: 2019 (series length 2099)\n",
      "Warning [3F4, f=1]: B/H/T lengths differ (B=17704848, H=17704848, T=553) -> truncating to 553\n",
      "[3F4, f=1] windows added: 473 (series length 553)\n",
      "Warning [3F4, f=2]: B/H/T lengths differ (B=11889504, H=11889504, T=594) -> truncating to 594\n",
      "[3F4, f=2] windows added: 514 (series length 594)\n",
      "Warning [3F4, f=3]: B/H/T lengths differ (B=20723472, H=20723472, T=1617) -> truncating to 1617\n",
      "[3F4, f=3] windows added: 1537 (series length 1617)\n",
      "Warning [3F4, f=4]: B/H/T lengths differ (B=16326555, H=16326555, T=2037) -> truncating to 2037\n",
      "[3F4, f=4] windows added: 1957 (series length 2037)\n",
      "Warning [3F4, f=5]: B/H/T lengths differ (B=10511792, H=10511792, T=2099) -> truncating to 2099\n",
      "[3F4, f=5] windows added: 2019 (series length 2099)\n",
      "Warning [3F4, f=6]: B/H/T lengths differ (B=6753600, H=6753600, T=2100) -> truncating to 2100\n",
      "[3F4, f=6] windows added: 2020 (series length 2100)\n",
      "Warning [3F4, f=7]: B/H/T lengths differ (B=4233600, H=4233600, T=2100) -> truncating to 2100\n",
      "[3F4, f=7] windows added: 2020 (series length 2100)\n",
      "Warning [77, f=1]: B/H/T lengths differ (B=11845920, H=11845920, T=370) -> truncating to 370\n",
      "[77, f=1] windows added: 290 (series length 370)\n",
      "Warning [77, f=2]: B/H/T lengths differ (B=27942336, H=27942336, T=1396) -> truncating to 1396\n",
      "[77, f=2] windows added: 1316 (series length 1396)\n",
      "Warning [77, f=3]: B/H/T lengths differ (B=22376736, H=22376736, T=1746) -> truncating to 1746\n",
      "[77, f=3] windows added: 1666 (series length 1746)\n",
      "Warning [77, f=4]: B/H/T lengths differ (B=16807455, H=16807455, T=2097) -> truncating to 2097\n",
      "[77, f=4] windows added: 2017 (series length 2097)\n",
      "Warning [77, f=5]: B/H/T lengths differ (B=10516800, H=10516800, T=2100) -> truncating to 2100\n",
      "[77, f=5] windows added: 2020 (series length 2100)\n",
      "Warning [77, f=6]: B/H/T lengths differ (B=6753600, H=6753600, T=2100) -> truncating to 2100\n",
      "[77, f=6] windows added: 2020 (series length 2100)\n",
      "Warning [77, f=7]: B/H/T lengths differ (B=4233600, H=4233600, T=2100) -> truncating to 2100\n",
      "[77, f=7] windows added: 2020 (series length 2100)\n",
      "Skipping [78, f=1]: missing B/H/T CSVs\n",
      "Warning [78, f=2]: B/H/T lengths differ (B=16313040, H=16313040, T=815) -> truncating to 815\n",
      "[78, f=2] windows added: 735 (series length 815)\n",
      "Warning [78, f=3]: B/H/T lengths differ (B=15251040, H=15251040, T=1190) -> truncating to 1190\n",
      "[78, f=3] windows added: 1110 (series length 1190)\n",
      "Warning [78, f=4]: B/H/T lengths differ (B=16823485, H=16823485, T=2099) -> truncating to 2099\n",
      "[78, f=4] windows added: 2019 (series length 2099)\n",
      "Warning [78, f=5]: B/H/T lengths differ (B=10516800, H=10516800, T=2100) -> truncating to 2100\n",
      "[78, f=5] windows added: 2020 (series length 2100)\n",
      "Warning [78, f=6]: B/H/T lengths differ (B=6753600, H=6753600, T=2100) -> truncating to 2100\n",
      "[78, f=6] windows added: 2020 (series length 2100)\n",
      "Warning [78, f=7]: B/H/T lengths differ (B=4233600, H=4233600, T=2100) -> truncating to 2100\n",
      "[78, f=7] windows added: 2020 (series length 2100)\n",
      "Warning [N27, f=1]: B/H/T lengths differ (B=28814400, H=28814400, T=900) -> truncating to 900\n",
      "[N27, f=1] windows added: 820 (series length 900)\n",
      "Warning [N27, f=2]: B/H/T lengths differ (B=2642112, H=2642112, T=132) -> truncating to 132\n",
      "[N27, f=2] windows added: 52 (series length 132)\n",
      "Warning [N27, f=3]: B/H/T lengths differ (B=13738752, H=13738752, T=1072) -> truncating to 1072\n",
      "[N27, f=3] windows added: 992 (series length 1072)\n",
      "Warning [N27, f=4]: B/H/T lengths differ (B=13946100, H=13946100, T=1740) -> truncating to 1740\n",
      "[N27, f=4] windows added: 1660 (series length 1740)\n",
      "Warning [N27, f=5]: B/H/T lengths differ (B=15024000, H=15024000, T=3000) -> truncating to 3000\n",
      "[N27, f=5] windows added: 2920 (series length 3000)\n",
      "Warning [N27, f=6]: B/H/T lengths differ (B=9648000, H=9648000, T=3000) -> truncating to 3000\n",
      "[N27, f=6] windows added: 2920 (series length 3000)\n",
      "Warning [N27, f=7]: B/H/T lengths differ (B=6048000, H=6048000, T=3000) -> truncating to 3000\n",
      "[N27, f=7] windows added: 2920 (series length 3000)\n",
      "Warning [N30, f=1]: B/H/T lengths differ (B=28782384, H=28782384, T=899) -> truncating to 899\n",
      "[N30, f=1] windows added: 819 (series length 899)\n",
      "Warning [N30, f=2]: B/H/T lengths differ (B=6425136, H=6425136, T=321) -> truncating to 321\n",
      "[N30, f=2] windows added: 241 (series length 321)\n",
      "Warning [N30, f=3]: B/H/T lengths differ (B=9765792, H=9765792, T=762) -> truncating to 762\n",
      "[N30, f=3] windows added: 682 (series length 762)\n",
      "Warning [N30, f=4]: B/H/T lengths differ (B=11004595, H=11004595, T=1373) -> truncating to 1373\n",
      "[N30, f=4] windows added: 1293 (series length 1373)\n",
      "Warning [N30, f=5]: B/H/T lengths differ (B=15024000, H=15024000, T=3000) -> truncating to 3000\n",
      "[N30, f=5] windows added: 2920 (series length 3000)\n",
      "Warning [N30, f=6]: B/H/T lengths differ (B=8844000, H=8844000, T=2750) -> truncating to 2750\n",
      "[N30, f=6] windows added: 2670 (series length 2750)\n",
      "Warning [N30, f=7]: B/H/T lengths differ (B=6048000, H=6048000, T=3000) -> truncating to 3000\n",
      "[N30, f=7] windows added: 2920 (series length 3000)\n",
      "Warning [N49, f=1]: B/H/T lengths differ (B=4229280, H=4229280, T=330) -> truncating to 330\n",
      "[N49, f=1] windows added: 250 (series length 330)\n",
      "Warning [N49, f=2]: B/H/T lengths differ (B=5850950, H=5850950, T=730) -> truncating to 730\n",
      "[N49, f=2] windows added: 650 (series length 730)\n",
      "Warning [N49, f=3]: B/H/T lengths differ (B=5879392, H=5879392, T=1174) -> truncating to 1174\n",
      "[N49, f=3] windows added: 1094 (series length 1174)\n",
      "Warning [N49, f=4]: B/H/T lengths differ (B=6653904, H=6653904, T=2069) -> truncating to 2069\n",
      "[N49, f=4] windows added: 1989 (series length 2069)\n",
      "Warning [N49, f=5]: B/H/T lengths differ (B=5820192, H=5820192, T=2887) -> truncating to 2887\n",
      "[N49, f=5] windows added: 2807 (series length 2887)\n",
      "Skipping [N49, f=6]: missing B/H/T CSVs\n",
      "Skipping [N49, f=7]: missing B/H/T CSVs\n",
      "Warning [N87, f=1]: B/H/T lengths differ (B=8612304, H=8612304, T=269) -> truncating to 269\n",
      "[N87, f=1] windows added: 189 (series length 269)\n",
      "Warning [N87, f=2]: B/H/T lengths differ (B=13050432, H=13050432, T=652) -> truncating to 652\n",
      "[N87, f=2] windows added: 572 (series length 652)\n",
      "Warning [N87, f=3]: B/H/T lengths differ (B=14456448, H=14456448, T=1128) -> truncating to 1128\n",
      "[N87, f=3] windows added: 1048 (series length 1128)\n",
      "Warning [N87, f=4]: B/H/T lengths differ (B=13657560, H=13657560, T=1704) -> truncating to 1704\n",
      "[N87, f=4] windows added: 1624 (series length 1704)\n",
      "Warning [N87, f=5]: B/H/T lengths differ (B=14392992, H=14392992, T=2874) -> truncating to 2874\n",
      "[N87, f=5] windows added: 2794 (series length 2874)\n",
      "Warning [N87, f=6]: B/H/T lengths differ (B=9648000, H=9648000, T=3000) -> truncating to 3000\n",
      "[N87, f=6] windows added: 2920 (series length 3000)\n",
      "Warning [N87, f=7]: B/H/T lengths differ (B=6027840, H=6027840, T=2990) -> truncating to 2990\n",
      "[N87, f=7] windows added: 2910 (series length 2990)\n",
      "\n",
      "Total samples in GlobalDataset: 111074\n",
      "\n",
      "========== Training GLOBAL model ==========\n",
      "Epoch   1/80  train_loss=0.091147  val_loss=0.011040\n",
      "Epoch   2/80  train_loss=0.007988  val_loss=0.005620\n",
      "Epoch   3/80  train_loss=0.004886  val_loss=0.004129\n",
      "Epoch   4/80  train_loss=0.003676  val_loss=0.003249\n",
      "Epoch   5/80  train_loss=0.002922  val_loss=0.002496\n",
      "Epoch   6/80  train_loss=0.002395  val_loss=0.002099\n",
      "Epoch   7/80  train_loss=0.002055  val_loss=0.001946\n",
      "Epoch   8/80  train_loss=0.001754  val_loss=0.001626\n",
      "Epoch   9/80  train_loss=0.001493  val_loss=0.001417\n",
      "Epoch  10/80  train_loss=0.001294  val_loss=0.001144\n",
      "Epoch  11/80  train_loss=0.001131  val_loss=0.001036\n",
      "Epoch  12/80  train_loss=0.001022  val_loss=0.000962\n",
      "Epoch  13/80  train_loss=0.000966  val_loss=0.000962\n",
      "Epoch  14/80  train_loss=0.000942  val_loss=0.000953\n",
      "Epoch  15/80  train_loss=0.000913  val_loss=0.000929\n",
      "Epoch  16/80  train_loss=0.000911  val_loss=0.000986\n",
      "Epoch  17/80  train_loss=0.000894  val_loss=0.000960\n",
      "Epoch  18/80  train_loss=0.000883  val_loss=0.000849\n",
      "Epoch  19/80  train_loss=0.000877  val_loss=0.000850\n",
      "Epoch  20/80  train_loss=0.000879  val_loss=0.000883\n",
      "Epoch  21/80  train_loss=0.000869  val_loss=0.000882\n",
      "Epoch  22/80  train_loss=0.000857  val_loss=0.000877\n",
      "Epoch  23/80  train_loss=0.000848  val_loss=0.000820\n",
      "Epoch  24/80  train_loss=0.000840  val_loss=0.001012\n",
      "Epoch  25/80  train_loss=0.000832  val_loss=0.000807\n",
      "Epoch  26/80  train_loss=0.000813  val_loss=0.000808\n",
      "Epoch  27/80  train_loss=0.000782  val_loss=0.000841\n",
      "Epoch  28/80  train_loss=0.000774  val_loss=0.000870\n",
      "Epoch  29/80  train_loss=0.000741  val_loss=0.000730\n",
      "Epoch  30/80  train_loss=0.000728  val_loss=0.000699\n",
      "Epoch  31/80  train_loss=0.000717  val_loss=0.000724\n",
      "Epoch  32/80  train_loss=0.000700  val_loss=0.000738\n",
      "Epoch  33/80  train_loss=0.000687  val_loss=0.000692\n",
      "Epoch  34/80  train_loss=0.000674  val_loss=0.000661\n",
      "Epoch  35/80  train_loss=0.000669  val_loss=0.000692\n",
      "Epoch  36/80  train_loss=0.000671  val_loss=0.000700\n",
      "Epoch  37/80  train_loss=0.000652  val_loss=0.000622\n",
      "Epoch  38/80  train_loss=0.000642  val_loss=0.000710\n",
      "Epoch  39/80  train_loss=0.000643  val_loss=0.000905\n",
      "Epoch  40/80  train_loss=0.000632  val_loss=0.000643\n",
      "Epoch  41/80  train_loss=0.000609  val_loss=0.000601\n",
      "Epoch  42/80  train_loss=0.000608  val_loss=0.000600\n",
      "Epoch  43/80  train_loss=0.000604  val_loss=0.000595\n",
      "Epoch  44/80  train_loss=0.000596  val_loss=0.000633\n",
      "Epoch  45/80  train_loss=0.000587  val_loss=0.000631\n",
      "Epoch  46/80  train_loss=0.000583  val_loss=0.000595\n",
      "Epoch  47/80  train_loss=0.000571  val_loss=0.000568\n",
      "Epoch  48/80  train_loss=0.000565  val_loss=0.000570\n",
      "Epoch  49/80  train_loss=0.000563  val_loss=0.000595\n",
      "Epoch  50/80  train_loss=0.000559  val_loss=0.000619\n",
      "Epoch  51/80  train_loss=0.000553  val_loss=0.000547\n",
      "Epoch  52/80  train_loss=0.000550  val_loss=0.000579\n",
      "Epoch  53/80  train_loss=0.000548  val_loss=0.000549\n",
      "Epoch  54/80  train_loss=0.000538  val_loss=0.000549\n",
      "Epoch  55/80  train_loss=0.000535  val_loss=0.000554\n",
      "Epoch  56/80  train_loss=0.000528  val_loss=0.000549\n",
      "Epoch  57/80  train_loss=0.000519  val_loss=0.000530\n",
      "Epoch  58/80  train_loss=0.000526  val_loss=0.000592\n",
      "Epoch  59/80  train_loss=0.000512  val_loss=0.000582\n",
      "Epoch  60/80  train_loss=0.000508  val_loss=0.000576\n",
      "Epoch  61/80  train_loss=0.000506  val_loss=0.000536\n",
      "Epoch  62/80  train_loss=0.000502  val_loss=0.000571\n",
      "Epoch  63/80  train_loss=0.000504  val_loss=0.000559\n",
      "Epoch  64/80  train_loss=0.000499  val_loss=0.000513\n",
      "Epoch  65/80  train_loss=0.000492  val_loss=0.000554\n",
      "Epoch  66/80  train_loss=0.000477  val_loss=0.000495\n",
      "Epoch  67/80  train_loss=0.000478  val_loss=0.000517\n",
      "Epoch  68/80  train_loss=0.000479  val_loss=0.000494\n",
      "Epoch  69/80  train_loss=0.000471  val_loss=0.000493\n",
      "Epoch  70/80  train_loss=0.000476  val_loss=0.000544\n",
      "Epoch  71/80  train_loss=0.000471  val_loss=0.000497\n",
      "Epoch  72/80  train_loss=0.000468  val_loss=0.000510\n",
      "Epoch  73/80  train_loss=0.000467  val_loss=0.000474\n",
      "Epoch  74/80  train_loss=0.000458  val_loss=0.000485\n",
      "Epoch  75/80  train_loss=0.000456  val_loss=0.000497\n",
      "Epoch  76/80  train_loss=0.000450  val_loss=0.000498\n",
      "Epoch  77/80  train_loss=0.000449  val_loss=0.000531\n",
      "Epoch  78/80  train_loss=0.000443  val_loss=0.000467\n",
      "Epoch  79/80  train_loss=0.000442  val_loss=0.000510\n",
      "Epoch  80/80  train_loss=0.000435  val_loss=0.000451\n",
      "\n",
      "===== GLOBAL METRICS =====\n",
      "Global RMSE: 0.1130 (H units)\n",
      "Global Relative Error: 0.75%\n",
      "\n",
      "===== Per-material, per-frequency metrics =====\n",
      "material  freq  n_samples   rmse  rel_err\n",
      "    3C90     1        567 0.0523   0.2047\n",
      "    3C90     2       1379 0.0272   0.3438\n",
      "    3C90     3       2172 0.0331   0.5079\n",
      "    3C90     4       3172 0.0510   0.4976\n",
      "    3C90     5       3189 0.0425   0.8377\n",
      "    3C90     6       3220 0.0875   1.5522\n",
      "    3C90     7       3010 0.2379   3.4820\n",
      "    3C94     1       1028 0.0278   0.2843\n",
      "    3C94     2        258 0.1057   0.1892\n",
      "    3C94     3        761 0.0462   0.2947\n",
      "    3C94     4       1979 0.0439   0.4939\n",
      "    3C94     5       1787 0.0384   0.9801\n",
      "    3C94     6       2020 0.0653   1.3769\n",
      "    3C94     7       2020 0.1996   3.3298\n",
      "     3E6     1        815 0.0093   0.2026\n",
      "     3E6     2       2015 0.0127   0.3508\n",
      "     3E6     3        142 0.0210   0.8211\n",
      "     3E6     4       1292 0.0376   0.9204\n",
      "     3E6     5       1720 0.0324   2.0429\n",
      "     3E6     6       2020 0.0528   2.7979\n",
      "     3E6     7       2019 0.1451   6.2449\n",
      "     3F4     1        473 0.0636   0.0559\n",
      "     3F4     2        514 0.0747   0.3001\n",
      "     3F4     3       1537 0.1470   0.2572\n",
      "     3F4     4       1957 0.1240   0.3838\n",
      "     3F4     5       2019 0.0700   0.8462\n",
      "     3F4     6       2020 0.1148   1.1685\n",
      "     3F4     7       2020 0.2517   2.1058\n",
      "      77     1        290 0.0312   0.2052\n",
      "      77     2       1316 0.0264   0.2872\n",
      "      77     3       1666 0.0310   0.2841\n",
      "      77     4       2017 0.0366   0.8810\n",
      "      77     5       2020 0.0429   1.0525\n",
      "      77     6       2020 0.0768   1.5574\n",
      "      77     7       2020 0.2187   4.4151\n",
      "      78     2        735 0.0199   0.3879\n",
      "      78     3       1110 0.0306   0.1958\n",
      "      78     4       2019 0.0388   0.4736\n",
      "      78     5       2020 0.0445   0.9011\n",
      "      78     6       2020 0.0927   1.7644\n",
      "      78     7       2020 0.2346   3.6571\n",
      "     N27     1        820 0.0605   0.5172\n",
      "     N27     2         52 0.0376   0.7167\n",
      "     N27     3        992 0.0535   0.7167\n",
      "     N27     4       1660 0.0526   0.2876\n",
      "     N27     5       2920 0.0368   0.8671\n",
      "     N27     6       2920 0.0588   1.1248\n",
      "     N27     7       2920 0.1846   3.1358\n",
      "     N30     1        819 0.0203   0.3479\n",
      "     N30     2        241 0.0270   0.2170\n",
      "     N30     3        682 0.0402   0.3557\n",
      "     N30     4       1293 0.0398   0.3316\n",
      "     N30     5       2920 0.0180   1.3084\n",
      "     N30     6       2670 0.0268   1.5292\n",
      "     N30     7       2920 0.0710   3.3712\n",
      "     N49     1        250 0.1939   0.7290\n",
      "     N49     2        650 0.1810   0.3841\n",
      "     N49     3       1094 0.3097   0.8497\n",
      "     N49     4       1989 0.3135   1.0691\n",
      "     N49     5       2807 0.0839   2.2102\n",
      "     N87     1        189 0.0237   0.1840\n",
      "     N87     2        572 0.0673   0.2881\n",
      "     N87     3       1048 0.0523   0.4338\n",
      "     N87     4       1624 0.0537   0.5049\n",
      "     N87     5       2794 0.1191   1.1273\n",
      "     N87     6       2920 0.0282   1.8867\n",
      "     N87     7       2910 0.0747   4.1401\n",
      "\n",
      "Saved global model to: global_lstm_model.pt\n",
      "Saved metrics to: global_results.csv\n",
      "Saved plots: global_train_val_loss.png, global_rmse_by_material.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Folder that contains subfolders: 3C90, 3C94, 3E6, 3F4, 77, 78, N27, N30, N49, N87\n",
    "# If you run this script from that folder, \".\" is fine.\n",
    "ROOT = \".\"  \n",
    "\n",
    "MATERIALS = [\"3C90\", \"3C94\", \"3E6\", \"3F4\", \"77\", \"78\", \"N27\", \"N30\", \"N49\", \"N87\"]\n",
    "FREQS = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "SEQ_LEN = 80\n",
    "STRIDE = 5\n",
    "BATCH_SIZE = 256      # you can drop to 128 if CPU is slow\n",
    "EPOCHS = 80\n",
    "LR = 1e-3\n",
    "PATIENCE = 20\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def load_1d(path: str, nrows: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load a 1D csv file (no header) as float32 np.array.\n",
    "    If nrows is given, only that many rows are read (to avoid massive I/O).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=None, nrows=nrows)\n",
    "    arr = df.values.squeeze().astype(np.float32)\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dataset: all materials + all frequencies\n",
    "# ============================================================\n",
    "\n",
    "class GlobalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Builds a single dataset over all materials and all frequencies.\n",
    "\n",
    "    For each (material, freq):\n",
    "      - read T fully (short)\n",
    "      - read only len(T) rows from B and H\n",
    "      - truncate if B/H/T still mismatched\n",
    "      - compute dB/dt\n",
    "      - create sliding windows of length SEQ_LEN, stride=1 (we don't\n",
    "        actually use STRIDE here; datasets are already big)\n",
    "\n",
    "    Each sample:\n",
    "      x      : (SEQ_LEN, 3)  [B_norm, H_norm, dBdt_norm]\n",
    "      mat_id : int in [0, n_materials)\n",
    "      freq_id: int in [0, n_freqs)\n",
    "      y      : scalar H_norm at (t+SEQ_LEN)\n",
    "      H_mean, H_std: to denormalize later (per-series stats)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, materials, freqs, seq_len=80, stride=5):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self.mat_ids = []\n",
    "        self.freq_ids = []\n",
    "        self.H_means = []\n",
    "        self.H_stds = []\n",
    "\n",
    "        for m_idx, mat in enumerate(materials):\n",
    "            for f_idx, freq in enumerate(freqs):\n",
    "                B_path = os.path.join(root, mat, f\"{mat}_{freq}_B.csv\")\n",
    "                H_path = os.path.join(root, mat, f\"{mat}_{freq}_H.csv\")\n",
    "                T_path = os.path.join(root, mat, f\"{mat}_{freq}_T.csv\")\n",
    "\n",
    "                if not (os.path.exists(B_path) and os.path.exists(H_path) and os.path.exists(T_path)):\n",
    "                    print(f\"Skipping [{mat}, f={freq}]: missing B/H/T CSVs\")\n",
    "                    continue\n",
    "\n",
    "                # Read T fully (short), then only len(T) rows from B/H\n",
    "                T = load_1d(T_path)\n",
    "                lenT = len(T)\n",
    "\n",
    "                B = load_1d(B_path, nrows=lenT)\n",
    "                H = load_1d(H_path, nrows=lenT)\n",
    "\n",
    "                lenB, lenH = len(B), len(H)\n",
    "                if not (lenB == lenH == lenT):\n",
    "                    min_len = min(lenB, lenH, lenT)\n",
    "                    print(\n",
    "                        f\"Warning [{mat}, f={freq}]: \"\n",
    "                        f\"B/H/T lengths differ (B={lenB}, H={lenH}, T={lenT}) \"\n",
    "                        f\"-> truncating to {min_len}\"\n",
    "                    )\n",
    "                    B = B[:min_len]\n",
    "                    H = H[:min_len]\n",
    "                    T = T[:min_len]\n",
    "\n",
    "                # safety\n",
    "                B = np.asarray(B, dtype=np.float32).reshape(-1)\n",
    "                H = np.asarray(H, dtype=np.float32).reshape(-1)\n",
    "                dBdt = np.gradient(B).astype(np.float32)\n",
    "\n",
    "                B_mean, B_std = float(B.mean()), float(B.std() + 1e-8)\n",
    "                H_mean, H_std = float(H.mean()), float(H.std() + 1e-8)\n",
    "                dB_mean, dB_std = float(dBdt.mean()), float(dBdt.std() + 1e-8)\n",
    "\n",
    "                Bn = (B - B_mean) / B_std\n",
    "                Hn = (H - H_mean) / H_std\n",
    "                dBn = (dBdt - dB_mean) / dB_std\n",
    "\n",
    "                n = len(Bn)\n",
    "                count_windows = 0\n",
    "\n",
    "                for start in range(0, n - seq_len):\n",
    "                    end = start + seq_len\n",
    "                    if end >= n:\n",
    "                        break\n",
    "\n",
    "                    x = np.stack([Bn[start:end], Hn[start:end], dBn[start:end]], axis=-1)\n",
    "                    y = Hn[end]\n",
    "\n",
    "                    self.inputs.append(x.astype(np.float32))\n",
    "                    self.targets.append(np.float32(y))\n",
    "                    self.mat_ids.append(m_idx)\n",
    "                    self.freq_ids.append(f_idx)\n",
    "                    self.H_means.append(H_mean)\n",
    "                    self.H_stds.append(H_std)\n",
    "                    count_windows += 1\n",
    "\n",
    "                print(f\"[{mat}, f={freq}] windows added: {count_windows} (series length {n})\")\n",
    "\n",
    "        self.inputs = np.stack(self.inputs, axis=0)\n",
    "        self.targets = np.stack(self.targets, axis=0)\n",
    "        self.mat_ids = np.array(self.mat_ids, dtype=np.int64)\n",
    "        self.freq_ids = np.array(self.freq_ids, dtype=np.int64)\n",
    "        self.H_means = np.array(self.H_means, dtype=np.float32)\n",
    "        self.H_stds = np.array(self.H_stds, dtype=np.float32)\n",
    "\n",
    "        print(f\"\\nTotal samples in GlobalDataset: {len(self.inputs)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.inputs[idx])         # (L, 3)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        mat_id = torch.tensor(self.mat_ids[idx], dtype=torch.long)\n",
    "        freq_id = torch.tensor(self.freq_ids[idx], dtype=torch.long)\n",
    "        h_mean = torch.tensor(self.H_means[idx], dtype=torch.float32)\n",
    "        h_std = torch.tensor(self.H_stds[idx], dtype=torch.float32)\n",
    "        return x, mat_id, freq_id, y, h_mean, h_std\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model: shared LSTM + (material, freq) embeddings\n",
    "# ============================================================\n",
    "\n",
    "class GlobalModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_materials: int,\n",
    "        n_freqs: int,\n",
    "        input_dim: int = 3,\n",
    "        lstm_hidden: int = 32,\n",
    "        lstm_layers: int = 1,\n",
    "        mat_emb_dim: int = 4,\n",
    "        freq_emb_dim: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.mat_emb = nn.Embedding(n_materials, mat_emb_dim)\n",
    "        self.freq_emb = nn.Embedding(n_freqs, freq_emb_dim)\n",
    "\n",
    "        fc_in = lstm_hidden + mat_emb_dim + freq_emb_dim\n",
    "        self.fc = nn.Linear(fc_in, 1)\n",
    "\n",
    "    def forward(self, x, mat_id, freq_id):\n",
    "        # x: (B, L, 3)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last = lstm_out[:, -1, :]  # (B, hidden)\n",
    "\n",
    "        mat_e = self.mat_emb(mat_id)   # (B, mat_emb_dim)\n",
    "        freq_e = self.freq_emb(freq_id)  # (B, freq_emb_dim)\n",
    "\n",
    "        h = torch.cat([last, mat_e, freq_e], dim=-1)\n",
    "        y = self.fc(h).squeeze(-1)\n",
    "        return y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training + evaluation\n",
    "# ============================================================\n",
    "\n",
    "def train_global_model():\n",
    "    set_seed(42)\n",
    "\n",
    "    dataset = GlobalDataset(ROOT, MATERIALS, FREQS, seq_len=SEQ_LEN, stride=STRIDE)\n",
    "    n_total = len(dataset)\n",
    "    n_train = int(0.8 * n_total)\n",
    "    n_val = n_total - n_train\n",
    "    train_ds, val_ds = random_split(\n",
    "        dataset, [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = GlobalModel(\n",
    "        n_materials=len(MATERIALS),\n",
    "        n_freqs=len(FREQS),\n",
    "        input_dim=3,\n",
    "        lstm_hidden=32,\n",
    "        lstm_layers=1,\n",
    "        mat_emb_dim=4,\n",
    "        freq_emb_dim=2,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "\n",
    "    print(\"\\n========== Training GLOBAL model ==========\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # ---- train\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, mat_idb, freq_idb, yb, hmean_b, hstd_b in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            mat_idb = mat_idb.to(DEVICE)\n",
    "            freq_idb = freq_idb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb, mat_idb, freq_idb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # ---- val\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, mat_idb, freq_idb, yb, hmean_b, hstd_b in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                mat_idb = mat_idb.to(DEVICE)\n",
    "                freq_idb = freq_idb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "\n",
    "                preds = model(xb, mat_idb, freq_idb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        train_loss = float(np.mean(train_losses))\n",
    "        val_loss = float(np.mean(val_losses))\n",
    "        train_history.append(train_loss)\n",
    "        val_history.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS}  train_loss={train_loss:.6f}  val_loss={val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            best_val = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best val_loss={best_val:.6f}\")\n",
    "                break\n",
    "\n",
    "    # load best weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # ---- Evaluate on ALL samples in physical units\n",
    "    full_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    preds_norm_all = []\n",
    "    targets_norm_all = []\n",
    "    mat_ids_all = []\n",
    "    freq_ids_all = []\n",
    "    H_means_all = []\n",
    "    H_stds_all = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, mat_idb, freq_idb, yb, hmean_b, hstd_b in full_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            mat_idb = mat_idb.to(DEVICE)\n",
    "            freq_idb = freq_idb.to(DEVICE)\n",
    "\n",
    "            preds = model(xb, mat_idb, freq_idb).cpu().numpy()\n",
    "            y_np = yb.numpy()\n",
    "\n",
    "            preds_norm_all.append(preds)\n",
    "            targets_norm_all.append(y_np)\n",
    "            mat_ids_all.append(mat_idb.cpu().numpy())\n",
    "            freq_ids_all.append(freq_idb.cpu().numpy())\n",
    "            H_means_all.append(hmean_b.numpy())\n",
    "            H_stds_all.append(hstd_b.numpy())\n",
    "\n",
    "    preds_norm_all = np.concatenate(preds_norm_all)\n",
    "    targets_norm_all = np.concatenate(targets_norm_all)\n",
    "    mat_ids_all = np.concatenate(mat_ids_all)\n",
    "    freq_ids_all = np.concatenate(freq_ids_all)\n",
    "    H_means_all = np.concatenate(H_means_all)\n",
    "    H_stds_all = np.concatenate(H_stds_all)\n",
    "\n",
    "    preds_phys_all = preds_norm_all * H_stds_all + H_means_all\n",
    "    targets_phys_all = targets_norm_all * H_stds_all + H_means_all\n",
    "\n",
    "    # global metrics\n",
    "    global_rmse = float(np.sqrt(np.mean((preds_phys_all - targets_phys_all) ** 2)))\n",
    "    rms_meas_global = float(np.sqrt(np.mean(targets_phys_all ** 2)))\n",
    "    global_rel = global_rmse / (rms_meas_global + 1e-12) * 100.0\n",
    "\n",
    "    print(\"\\n===== GLOBAL METRICS =====\")\n",
    "    print(f\"Global RMSE: {global_rmse:.4f} (H units)\")\n",
    "    print(f\"Global Relative Error: {global_rel:.2f}%\")\n",
    "\n",
    "    # ---- per-material & per-frequency metrics\n",
    "    rows = []\n",
    "    for m_idx, mat in enumerate(MATERIALS):\n",
    "        for f_idx, freq in enumerate(FREQS):\n",
    "            mask = (mat_ids_all == m_idx) & (freq_ids_all == f_idx)\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            p = preds_phys_all[mask]\n",
    "            t = targets_phys_all[mask]\n",
    "            rmse = float(np.sqrt(np.mean((p - t) ** 2)))\n",
    "            rms_meas = float(np.sqrt(np.mean(t ** 2)))\n",
    "            rel = rmse / (rms_meas + 1e-12) * 100.0\n",
    "\n",
    "            rows.append({\n",
    "                \"material\": mat,\n",
    "                \"freq\": freq,\n",
    "                \"n_samples\": int(mask.sum()),\n",
    "                \"rmse\": rmse,\n",
    "                \"rel_err\": rel,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(\"global_results.csv\", index=False)\n",
    "\n",
    "    print(\"\\n===== Per-material, per-frequency metrics =====\")\n",
    "    print(df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "    # ---- plots\n",
    "    # 1) Train vs val loss\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    epochs = np.arange(1, len(train_history) + 1)\n",
    "    plt.plot(epochs, train_history, label=\"train\")\n",
    "    plt.plot(epochs, val_history, label=\"val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.title(\"Global model: train vs val loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"global_train_val_loss.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Average RMSE per material\n",
    "    mat_rmse = df.groupby(\"material\")[\"rmse\"].mean().reset_index()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(mat_rmse[\"material\"], mat_rmse[\"rmse\"])\n",
    "    plt.xlabel(\"Material\")\n",
    "    plt.ylabel(\"RMSE (H units)\")\n",
    "    plt.title(\"Average RMSE per material (across frequencies)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"global_rmse_by_material.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # save model\n",
    "    torch.save(model.state_dict(), \"global_lstm_model.pt\")\n",
    "    print(\"\\nSaved global model to: global_lstm_model.pt\")\n",
    "    print(\"Saved metrics to: global_results.csv\")\n",
    "    print(\"Saved plots: global_train_val_loss.png, global_rmse_by_material.png\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    train_global_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374fa6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf74b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
